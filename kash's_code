# -*- coding: utf-8 -*-
"""
Created on Tue Oct 24 18:10:57 2017

@author: akash
"""
########################################
# ENVIRONMENT PREP
import os
### Provide the path here
os.chdir('C:\\Users\\akash\\Desktop\\GWU\\6103_DataMining_FBradley\\Project_1') 

### Packages
import pandas as pd
import numpy as np

## DataLoad and Global Filtering

### read in the 2009 survey data from CSV 
complete_energy_df  = pd.read_csv('recs2009_public.csv')
keys_df = pd.read_csv('public_layout.csv')

### Create More Managable DataFrame
energy_df = complete_energy_df[['KWH','WALLTYPE','ROOFTYPE','YEARMADE','AIA_Zone','BEDROOMS',
                          'ADQINSUL','TYPEGLASS', 'WINDOWS','DOOR1SUM']]

########################################
#Jim's Choices


# remove all records where negative values exist
energy_df = energy_df[(energy_df >= 0).all(1)]

# create KWH_range values to group the energy usage into ranges
conditions = [
    (energy_df['KWH'] >= 0) & (energy_df['KWH'] < 1000),
    (energy_df['KWH'] >= 2000) & (energy_df['KWH'] < 4000),
    (energy_df['KWH'] >= 4000) & (energy_df['KWH'] < 6000),
    (energy_df['KWH'] >= 6000) & (energy_df['KWH'] < 8000),
    (energy_df['KWH'] >= 8000) & (energy_df['KWH'] < 10000),
    (energy_df['KWH'] >= 10000) & (energy_df['KWH'] < 12000),
    (energy_df['KWH'] >= 12000) & (energy_df['KWH'] < 14000),
    (energy_df['KWH'] >= 14000) & (energy_df['KWH'] < 16000),
    (energy_df['KWH'] >= 16000) & (energy_df['KWH'] < 18000),
    (energy_df['KWH'] >= 18000) & (energy_df['KWH'] < 20000),
    (energy_df['KWH'] >= 20000)]
choices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
energy_df['KWH_range'] = np.select(conditions, choices, default=10)

energy_minusTARGET_df = energy_df[['WALLTYPE','ROOFTYPE','YEARMADE','AIA_Zone','BEDROOMS',
                          'ADQINSUL','TYPEGLASS', 'WINDOWS','DOOR1SUM']]


target_energy_df = energy_df[['KWH_range']]


########################################
#MODELING (DECISION TREE)
from sklearn.model_selection import train_test_split as tts
from sklearn.tree import DecisionTreeRegressor
from sklearn.cross_validation import cross_val_score
import matplotlib.pyplot as plt

## split data into training set & test set for Decision Tree
X_train, X_test, Y_train, Y_test =tts(energy_minusTARGET_df, target_energy_df, test_size = 0.3, random_state=6103)


### X are the features, y are the target KWH

## list of values to try
max_depth_range = range(1, 12)
### list to store the average MSE for each value of max_depth
all_MSE_scores = []

## calculate MSE score for each value of max_depth
for depth in max_depth_range:
    treereg = DecisionTreeRegressor(max_depth=depth, random_state=6103)
    MSE_scores = cross_val_score(treereg, X_train, Y_train, cv=14, scoring='neg_mean_squared_error')
    all_MSE_scores.append(np.mean(np.sqrt(-MSE_scores)))
    
## plot max_depth (x-axis) versus MSE (y-axis)
plt.plot(max_depth_range, all_MSE_scores)
plt.xlabel('max_depth')
plt.ylabel('MSE (lower is better)')

# Depth of 6

# max_depth=6 was best, so fit a tree using that parameter
treereg = DecisionTreeRegressor(max_depth=6, random_state=6103)
treereg.fit(X_train, Y_train)

# "Gini importance" of each feature: 
#    the (normalized) total reduction of error brought by that feature
## Feature_Cols 
feature_cols = list(energy_minusTARGET_df)
print(pd.DataFrame({'feature':feature_cols, 'importance':sorted(treereg.feature_importances_ *1000, reverse = True)}))


########################################
# DATA FILTERING 
### Create a Dataframe Based on Results from Decision Tree Model
data_energy_df = energy_minusTARGET_df[['WALLTYPE','ROOFTYPE','YEARMADE',
                               'AIA_Zone','BEDROOMS',
                               'ADQINSUL','TYPEGLASS',
                               'WINDOWS','DOOR1SUM']]

## Feature_Cols Based on Results from Decision Tree Model
feature_cols = list(data_energy_df)

## split data into training set & test set
X_train, X_test, Y_train, Y_test =tts(data_energy_df, target_energy_df, test_size = 0.3, random_state=6103)

'''
### create a dataframe that contains only the columns we care about
limit_energy_df = energy_df[['WALLTYPE','ROOFTYPE','YEARMADE',
                               'AIA_Zone','BEDROOMS',
                               'ADQINSUL','TYPEGLASS',
                               'WINDOWS','DOOR1SUM', 'KWH']]
'''
########################################
# EDA
from pandas.tools.plotting import scatter_matrix

##DV {KWH} Summary
plt.hist(energy_df.KWH, bins=1000)
plt.savefig('hist_1.png')

plt.hist(energy_df.KWH_range)
plt.savefig('hist_2.png')

plt.figure()
plt.boxplot(energy_df.KWH, 0, 'gD')

         
##Make a scatter plot
'''
pd.plotting.scatter_matrix(limit_energy_df, 
                           c='KWH', 
                           figsize=(12, 12), 
                           marker='o', 
                           hist_kwds={'bins': 30}, 
                           s=30, 
                           alpha=.8, 
                           cmap='winter')
'''
'''
## make a Scatterplot 2
axes = pd.tools.plotting.scatter_matrix(limit_energy_df, alpha=0.2)
plt.tight_layout()
plt.savefig('scatter_matrix2.png')
'''

## make a Scatterplot 3 ,-- My FAVE
scatter_matrix(energy_df, alpha=0.2, figsize=(6, 6), diagonal='kde')
plt.savefig('scatter_matrix3.png')

#
del(energy_df)
#del(limit_energy_df)
########################################
#MODELING (KNN)
from sklearn.neighbors import KNeighborsClassifier

## knn tuning --> What K should we pick
### capture results
train_accuracy = []
test_accuracy  = []

### set kNN setting from 1 to 11
kNN_range = range(1, 11)
for neighbors in kNN_range:
  # start Nearest Neighbors Classifier with K of 1
  knn = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski', p=2)
  # train the data using Nearest Neighbors
  knn.fit(X_train, Y_train)
  # capture training accuracy
  train_accuracy.append(knn.score(X_train, Y_train))
  # predict using the test dataset  
  Y_pred = knn.predict(X_test)
  # capture test accuracy
  test_accuracy.append(knn.score(X_test, Y_test))

### plot results  
plt.plot(kNN_range, train_accuracy, label='training accuracy')
plt.plot(kNN_range, test_accuracy,  label='test accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Neighbors')
plt.legend()

## start Nearest Neighbors Classifier with K of 7
knn = KNeighborsClassifier(n_neighbors=7, metric='minkowski', p=2)

### train the data using Nearest Neighbors
knn.fit(X_train, Y_train)

### Model accuracy
Y_pred= knn.predict(X_test)
print('\nPrediction from X_test:')
print(Y_pred)

score = knn.score(X_test, Y_test)
print('Score:', score*100,'%')

### prediction
X_new = np.array([[2, 2, 2000, 2, 2,2,40,2,2]])
prediction = knn.predict(X_new)
print('Prediction:', prediction,'KWH')
########################################
# 
'''
##A-a-ron's Random Forest
from sklearn.ensemble import RandomForestClassifier
labels=data_energy_df.columns[:]
XX = data_energy_df.iloc[:, 1:].values
YY = data_energy_df.iloc[:, 0].values
forest = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)
forest.fit(XX,YY)
'''

## Do we need one-hot encoding
## Do we need to impute or remove nulls (-2)
## Do we need extra variables (ie: sqft)
## Create 10x10 Scatter plot with x-axis = KWH
## Front-end design if possible 
## Figure out Accuracy of Data (use IRIS Slide 14/24)
## Binning process for KWH
## Remove Outliers maybe
